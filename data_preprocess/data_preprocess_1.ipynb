{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe428a9e",
   "metadata": {},
   "source": [
    "下载**原始**数据\n",
    "\n",
    "+ 原地址：https://nlp.stanford.edu/pubs/stock-event.html\n",
    "+ 百度网盘地址：链接：https://pan.baidu.com/s/1QdQTDLNhAKYJnZP-nc9jhw?pwd=1t22 \n",
    "提取码：1t22\n",
    "\n",
    "注：正在运行的简化版项目不用下载原始数据。\n",
    "\n",
    "参考文献\n",
    "\n",
    "Lee, Heeyoung, Mihai Surdeanu, Bill MacCartney, and Dan Jurafsky. 2014. “On the Importance of Text Analysis for Stock Price Prediction.” In Proceedings of the 9th International Conference on Language Resources and Evaluation, LREC 2014, 1170–75."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9273b1f0",
   "metadata": {},
   "source": [
    "# 解压缩文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52dc69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import tarfile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbfd541",
   "metadata": {},
   "source": [
    "定义数据集文件夹路径`data_fp`，保证里面有以下文件：\n",
    "\n",
    "+ `8K.tar.gz`\n",
    "+ `EPS.tar.gz`\n",
    "+ `price_history.tar.gz`\n",
    "+ `snp_list.tar.gz`\n",
    "\n",
    "以上原始文件共占1.85GB硬盘空间，解压缩过后又会占1.84GB硬盘空间。\n",
    "\n",
    "注：正在运行的简化版项目压缩包内已经包含这些文件的简化版。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0be69f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fp = Path(\"C:/Users/xh_z/SynologyDrive/Projects/FinTechCaseStudies/CaseStudy3Sim\")\n",
    "data_fp_unzipped = data_fp/'.unzipped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ebc602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8K.tar.gz', 'EPS.tar.gz', 'price_history.tar.gz', 'snp_list.tar.gz']\n"
     ]
    }
   ],
   "source": [
    "print([file for file in os.listdir(data_fp) if file.endswith('.tar.gz')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d6677b",
   "metadata": {},
   "source": [
    "解压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f4c8295",
   "metadata": {},
   "outputs": [],
   "source": [
    "gz_files = ['8K.tar.gz', 'EPS.tar.gz', 'price_history.tar.gz', 'snp_list.tar.gz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e25dd797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 8K.tar.gz\n",
      "processing EPS.tar.gz\n",
      "processing price_history.tar.gz\n",
      "processing snp_list.tar.gz\n"
     ]
    }
   ],
   "source": [
    "for gz_file in gz_files:\n",
    "    with tarfile.open(data_fp/gz_file) as f:\n",
    "        print(f'processing {gz_file}')\n",
    "        f.extractall(data_fp_unzipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d37a7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL.gz']\n"
     ]
    }
   ],
   "source": [
    "# every file in the archive '8K.tar.gz' is an archive on its own\n",
    "# and has been extracted to subfolder \"8K-gz\"\n",
    "path_8K = data_fp_unzipped/'8K-gz'\n",
    "gz_files_8K = [file for file in os.listdir(path_8K) if file.endswith('.gz')]\n",
    "print(gz_files_8K[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b8819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gz_file_8K in gz_files_8K:\n",
    "#     with gzip.open(path_8K/gz_file_8K, 'rb') as f:\n",
    "#         file_content = f.read()\n",
    "#     save_as = path_8K/gz_file_8K.replace('.gz', '.txt')\n",
    "#     with open(save_as, 'w', encoding='utf-8') as f:\n",
    "#         f.write(file_content.decode('utf-8'))\n",
    "#     os.remove(path_8K/gz_file_8K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e618ce6",
   "metadata": {},
   "source": [
    "# 文本预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceec1e5",
   "metadata": {},
   "source": [
    "解压缩后得到的文件夹`8K-gz`内，是部分标普1500公司从2002年到2012年的8K文本。每个公司对应一个压缩`*.gz`文件，以该公司的股票代码命名。进一步解压缩这个文件，可以得到一个纯文本文件，由该公司2002至2012年之间内提交的所有8K文档拼接得到。每个8K文档包含在一个`<DOCUMENT></DOCUMENT>`标签里。可查看样例文件`AAPL.xml`。\n",
    "\n",
    "我们先将每个8K文档抽取出来，然后解析文档内容，抽取提交时间、内容类型等元数据，然后对正文部分进行一些删减，只保留有实质含义的内容，最后对正文部分进行分词。\n",
    "\n",
    "这一步的输出会存储在子文件夹`processed`里。对于原始文件，该文件夹最后大小为5.62 GB。\n",
    "\n",
    "+ 安装`spaCy`，用于文本预处理，参照[https://spacy.io/usage](https://spacy.io/usage)。我使用的指令：\n",
    "```\n",
    "conda install -c conda-forge spacy\n",
    "conda install -c conda-forge cupy\n",
    "python -m spacy download zh_core_web_sm\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "注：`spaCy`可以使用GPU来加速计算，但是需要自行安装cuda（PyTorch自带的cuda不可以）。cuda下载地址：[https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads)。\n",
    "\n",
    "+ 安装[BeautifulSoup4](https://beautiful-soup-4.readthedocs.io/en/latest/#installing-beautiful-soup)，同时安装`lxml`。\n",
    "```\n",
    "pip install beautifulsoup4 lxml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b68b0886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d794989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "626f34c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "nlp.max_length = nlp.max_length*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcccccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_one_doc(doc):\n",
    "    line_l = doc.split('\\n')\n",
    "    # step 1: extract meta data entries and the 'TEXT' entry\n",
    "    # this step produces `toc`\n",
    "    # which stands for 'table of contents', and is a list of section titles\n",
    "    toc = [] \n",
    "    for i, line in enumerate(line_l):\n",
    "        if line.startswith('TIME:'):\n",
    "            time_str = line[5:].strip()\n",
    "        elif line.startswith('EVENTS:'):\n",
    "            etypes = line[7:].strip().split('\\t')\n",
    "        elif line.startswith('TEXT:'):\n",
    "            # lines after this line is the filing content\n",
    "            text_lines = line_l[(i+1):]\n",
    "            for j, line in enumerate(text_lines):\n",
    "                if line.startswith('ITEM:'):\n",
    "                    # first several lines in filing content indicate filing items\n",
    "                    # keep them in `toc`\n",
    "                    toc.append(line[5:].strip())\n",
    "                else:\n",
    "                    # substantial filing content starts from the next line\n",
    "                    content_lines = [line.strip() for line in text_lines[(j+1):] if len(line.strip())>0]\n",
    "                    break\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # step 2: drop stereotyped lines\n",
    "    dropped = [False] * len(content_lines)\n",
    "    for i, line in enumerate(content_lines):\n",
    "        if not dropped[i]:\n",
    "            if line.startswith('Table of Contents'):\n",
    "                dropped[i] = True\n",
    "            elif line.startswith('Check the appropriate box below if the Form 8-K'):\n",
    "                dropped[i] = True\n",
    "                # also check the next a few lines\n",
    "                # if two consecutive lines correspond to a checkbox, then drop them\n",
    "                for j in range(i+2, min(i+2*10, len(content_lines)), 2):\n",
    "                    if content_lines[j] == 'o':\n",
    "                        dropped[j-1] = True\n",
    "                        dropped[j] = True          \n",
    "            else:\n",
    "                continue\n",
    "    content_lines = [line for line, is_dropped in zip(content_lines, dropped) if not is_dropped]\n",
    "    n = len(content_lines)\n",
    "    \n",
    "    # step 3: seperate doc into sections\n",
    "    toc_spans = [[None, None] for _ in toc]\n",
    "    sec_i = 0\n",
    "    sec = toc[sec_i].lower()\n",
    "    for i, line in enumerate(content_lines):\n",
    "        l = line.lower()\n",
    "        if l.endswith(sec) or l.endswith(sec+'.'):\n",
    "            # start of an item section\n",
    "            toc_spans[sec_i][0] = i\n",
    "            # also the end of a previous item section\n",
    "            if sec_i > 0:\n",
    "                toc_spans[sec_i-1][1] = i\n",
    "            sec_i += 1\n",
    "            if sec_i == len(toc):\n",
    "                # the last section ends with document\n",
    "                break\n",
    "            sec = toc[sec_i].lower()\n",
    "    toc_spans[-1][1] = n\n",
    "    for start, end in toc_spans:\n",
    "        try:\n",
    "            assert (start is not None) and (end is not None)\n",
    "        except AssertionError as e:\n",
    "            # simple strategy failed\n",
    "#             print(toc_spans, toc, '\\n*****\\n'.join(content_lines), sep='\\n')\n",
    "            body = {'all': ' '.join(content_lines)}\n",
    "            head = ''\n",
    "            seperated = False\n",
    "            break\n",
    "    else:\n",
    "        # pass all assertion tests\n",
    "        body = {sec: ' '.join(content_lines[span[0]:span[1]]) for sec, span in zip(toc, toc_spans)}\n",
    "        # keep the content not in any toc sections in `head`\n",
    "        head = ' '.join(content_lines[0:toc_spans[0][0]])\n",
    "        seperated = True\n",
    "    # step 4: save results\n",
    "    doc_d = {'body': body, 'head': head, \n",
    "             'time': time_str, 'events': etypes, 'toc': toc, \n",
    "             'toc_spans': toc_spans, 'seperated': seperated}\n",
    "    return doc_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2c4472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(doc, logging=False):\n",
    "    try:\n",
    "        doc = re.sub('\\s+', ' ', doc).strip()\n",
    "        doc = nlp(doc)\n",
    "        # tokenization, then only keep words\n",
    "        tokens = [tok.text.lower() for tok in doc if tok.is_alpha]\n",
    "        tokens = ' '.join(tokens)\n",
    "        return tokens\n",
    "    except:\n",
    "        print(doc)\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a04887cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = data_fp/'.processed'\n",
    "if not os.path.isdir(text_path):\n",
    "    os.mkdir(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "feb2a3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for gz_file_8K in tqdm(gz_files_8K):\n",
    "    firm = gz_file_8K.split('.')[0]\n",
    "    save_as = text_path/(firm+'.json')\n",
    "    if not os.path.isfile(save_as):\n",
    "        # unzip\n",
    "        with gzip.open(path_8K/gz_file_8K, 'rb') as f:\n",
    "            file_content = f.read().decode('ascii')\n",
    "        # extract 8K documents\n",
    "        root_node = BeautifulSoup(file_content, 'lxml')\n",
    "        doc_l = [doc.text for doc in root_node.find_all('document')]\n",
    "        # for each 8K doc, perform text preprocessing pipelines\n",
    "        doc_d_l = []\n",
    "        for one_doc in doc_l:\n",
    "            doc_d = parse_one_doc(one_doc)\n",
    "            doc_d['body'] = {sec: cleanup_text(doc) for sec, doc in doc_d['body'].items()}\n",
    "            doc_d_l.append(doc_d)\n",
    "        # save to json\n",
    "        with open(save_as, 'w', encoding='utf-8') as f:\n",
    "            json.dump(doc_d_l, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b55359b",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# doc_d_l[0]['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd970f",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html -TagRemovePreprocessor.remove_cell_tags='{\"remove_cell\"}' data_preprocess_1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ea7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
